import ijson
import pandas as pd
from tqdm import tqdm
from pathlib import Path

def stream_matches_from_large_json(json_path):
    """
    ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ ê²½ê¸° ë‹¨ìœ„ë¡œ stream íŒŒì‹±
    """
    with open(json_path, 'r', encoding='utf-8') as f:
        for match_id, match_data in ijson.kvitems(f, ''):
            yield match_id, match_data


def extract_events(match_id, match_data):
    """
    í•´ë‹¹ ê²½ê¸°ì˜ ëª¨ë“  í”„ë ˆì„ì—ì„œ events ì¶”ì¶œ
    """
    frames = match_data.get("info", {}).get("frames", [])
    for frame in frames:
        timestamp = frame.get("timestamp")
        for event in frame.get("events", []):
            event["timestamp"] = timestamp//60000
            event["match_id"] = match_id
            yield event


def parse_large_json_to_csv(json_path, output_dir="output", save_every=100000):
    """
    ì „ì²´ JSONì—ì„œ eventsë¥¼ íŒŒì‹±í•´ batch ë‹¨ìœ„ë¡œ CSV ì €ì¥
    """
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    temp_rows = []
    batch = 0
    total_events = 0

    for match_id, match_data in tqdm(stream_matches_from_large_json(json_path), desc="Processing Matches"):
        for event in extract_events(match_id, match_data):
            temp_rows.append(event)
            total_events += 1

        # ì¼ì • ê°œìˆ˜ë§ˆë‹¤ ì €ì¥
        if len(temp_rows) >= save_every:
            df = pd.json_normalize(temp_rows)
            df.to_csv(f"{output_dir}/events_batch_{batch}.csv", index=False)
            print(f"âœ… Saved events_batch_{batch}.csv ({len(temp_rows)} rows)")
            temp_rows.clear()
            batch += 1

    # ë‚¨ì€ ë°ì´í„° ì €ì¥
    if temp_rows:
        df = pd.json_normalize(temp_rows)
        df.to_csv(f"{output_dir}/events_batch_{batch}.csv", index=False)
        print(f"âœ… Saved events_batch_{batch}.csv ({len(temp_rows)} rows)")

    print(f"\nğŸ‰ ì „ì²´ ì´ë²¤íŠ¸ íŒŒì‹± ì™„ë£Œ: {total_events} rows across {batch + 1} files")



# ì‹¤í–‰ (ì˜ˆì‹œ)
if __name__ == "__main__":
    parse_large_json_to_csv("matches_details_time.json")
